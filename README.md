
# ğŸ“ Q-Learning Grid Environment using Kivy

This project demonstrates a reinforcement learning environment using the **Q-Learning** algorithm in a 2D grid world. The agent learns to navigate the grid, collect rewards (strawberries ğŸ“), and avoid obstacles through trial and error. The environment is visualized in real time using the **Kivy** graphical interface.

---

## ğŸ§  Project Overview

- **Algorithm:** Q-Learning (model-free reinforcement learning)
- **Interface:** Graphical User Interface (GUI) built with Kivy
- **Goal:** Teach an agent to find the optimal path to collect all strawberries while avoiding negative reward zones
- **Grid Size:** 10x10 grid world with:
  - âœ… Green zone (starting point)
  - ğŸš« Red cells (obstacles with penalties)
  - ğŸ“ Strawberry cells (positive reward)
  - ğŸŸ¦ Blue cell (goal/final reward â€” optional)

---

## ğŸ“¸ Interface Screenshot

![Q-Learning Grid Screenshot](assets/image.png.png)
git push -u origin main

---

## âš™ï¸ Features

- Real-time training visualization
- Customizable training parameters:
  - Epsilon (exploration rate)
  - Learning rate (Î±)
  - Discount factor (Î³)
  - Number of episodes
- Save and import Q-tables
- Track reward over time using live plotting
- Show optimal learned path

---

## ğŸš€ Getting Started

### ğŸ”§ Prerequisites

Install the required dependencies:

```bash
pip install kivy matplotlib numpy
```

> âš ï¸ Make sure you have Python 3.7+ installed.

---

### ğŸ§ª Running the App

In your terminal, navigate to the project directory and run:

```bash
python main.py
```

---

## ğŸ›ï¸ Interface Guide

| Component              | Description                                      |
|------------------------|--------------------------------------------------|
| Start Training         | Starts Q-learning training process               |
| Save Q-table           | Saves the learned Q-table to a file              |
| Import Q-table         | Load a previously saved Q-table                  |
| Show Optimal Path      | Displays the optimal path after training         |
| Epsilon                | Controls exploration (0 = full exploitation)     |
| Learning Rate (Î±)      | How quickly the agent updates its Q-values       |
| Discount Factor (Î³)    | Future reward importance                         |
| Episodes               | Number of training episodes                      |

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ main.py              # Main Kivy GUI and logic
â”œâ”€â”€ q_learning.py        # Q-learning algorithm implementation
â”œâ”€â”€ environment.py       # Environment setup (grid, rewards, penalties)
â”œâ”€â”€ utils.py             # Helper functions (optional)
â”œâ”€â”€ assets/              # Image files like strawberries, icons...
â””â”€â”€ README.md            # Project description file
```

---

## ğŸ’¡ Concepts Used

- Q-Learning algorithm
- Îµ-greedy policy for exploration/exploitation
- Value iteration and reward maximization
- Real-time rendering using Kivy framework
- State-action table (Q-table) storage and update

---

## ğŸ› ï¸ To Do / Future Improvements

- Add multiple agent support
- Save training history as CSV
- Add more complex reward strategies
- Allow dynamic grid resizing

---

## ğŸ§‘â€ğŸ’» Author

 **Fatima**

Feel free to contribute, fork, or star â­ the repo!

---

## ğŸ“œ License

This project is licensed under the MIT License.
